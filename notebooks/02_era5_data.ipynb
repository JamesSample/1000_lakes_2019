{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import nivapy3 as nivapy\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "from calendar import monthrange\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to RESA\n",
    "eng = nivapy.da.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1000 Lakes 2019 (Part 2: Process ERA5 data)\n",
    "\n",
    "This notebook extracts ERA5 data for temperature, precipitation and runoff for the 1000 Lakes stations. The ERA5 dataset was downloaded from Copernicus [here](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels-monthly-means?tab=overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get station co-ordinates \n",
    "\n",
    "The relevant project ID in RESA is 4530."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stn_df = nivapy.da.select_resa_project_stations([4530], eng)\n",
    "stn_df.dropna(subset=[\"latitude\", \"longitude\"], inplace=True)\n",
    "print(len(stn_df), \"station in project.\")\n",
    "\n",
    "# Save for use on Hub\n",
    "csv_path = r'../output/resa_1000_lakes_stn_list.csv'\n",
    "stn_df.to_csv(csv_path, index=False)\n",
    "\n",
    "stn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map\n",
    "nivapy.spatial.quickmap(\n",
    "    stn_df, popup=\"station_code\", cluster=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ERA5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read nc\n",
    "nc_path = r\"../../../era5_tmp_pptn_runoff_1979-2019_monthly.nc\"\n",
    "ds = xr.open_dataset(nc_path)\n",
    "\n",
    "# 'expver' = 1 is up to end of 2019; 'expver' = 2 is for most recent data\n",
    "ds = ds.sel(expver=1).drop(\"expver\")\n",
    "\n",
    "ds = ds.load()\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Extract monthly data\n",
    "\n",
    "Values for `tp` and `ro` are in mm/day (for the month), which is slightly awkward (see [here](https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation#ERA5:datadocumentation-Monthlymeans)). This will be easier to deal with in `pandas` *after* extracting monthly series for the station co-ordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over sites\n",
    "df_list = []\n",
    "for idx, row in stn_df.iterrows():\n",
    "    lat = row[\"latitude\"]\n",
    "    lon = row[\"longitude\"]\n",
    "    stn_id = row[\"station_id\"]\n",
    "\n",
    "    # Get time series\n",
    "    df = (\n",
    "        ds.sel(latitude=lat, longitude=lon, method=\"nearest\")\n",
    "        .to_dataframe()\n",
    "        .reset_index()\n",
    "    )\n",
    "    df[\"station_id\"] = stn_id\n",
    "    df[\"year\"] = df[\"time\"].dt.year\n",
    "    df[\"month\"] = df[\"time\"].dt.month\n",
    "\n",
    "    df_list.append(df)\n",
    "\n",
    "# Combine results\n",
    "df = pd.concat(df_list, sort=True).reset_index(drop=True)\n",
    "df = df[[\"station_id\", \"year\", \"month\", \"t2m\", \"tp\", \"ro\"]]\n",
    "df = df.melt(id_vars=[\"station_id\", \"year\", \"month\"])\n",
    "\n",
    "# Exclude 2020 as incomplete\n",
    "df = df.query(\"year < 2020\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Aggregate to annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_in_month(row):\n",
    "    return monthrange(row[\"year\"], row[\"month\"])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add days in month\n",
    "df[\"days_in_month\"] = df.apply(days_in_month, axis=1)\n",
    "\n",
    "# Weight values by days in month\n",
    "df[\"month_accum\"] = df[\"value\"] * df[\"days_in_month\"]\n",
    "\n",
    "# Aggregate to annual\n",
    "df = df.groupby([\"station_id\", \"year\", \"variable\"]).sum().reset_index()\n",
    "\n",
    "# Calculate mean for temp; total for tp and ro; convert units\n",
    "df.loc[df[\"variable\"] == \"t2m\", \"month_accum\"] = (\n",
    "    df[\"month_accum\"] / df[\"days_in_month\"]\n",
    ") - 273.15  # K to C\n",
    "\n",
    "df.loc[df[\"variable\"] == \"ro\", \"month_accum\"] = df[\"month_accum\"] * 1000  # m to mm\n",
    "\n",
    "df.loc[df[\"variable\"] == \"tp\", \"month_accum\"] = df[\"month_accum\"] * 1000  # m to mm\n",
    "\n",
    "# Tidy\n",
    "df = df[[\"station_id\", \"year\", \"variable\", \"month_accum\"]]\n",
    "df.rename({\"month_accum\": \"value\"}, inplace=True, axis=1)\n",
    "\n",
    "# Some stations lie just \"offshore\" in ERA5, where 'ro' is always 0\n",
    "# Drop these rows\n",
    "df = df.query(\"not((value == 0) and (variable == 'ro'))\")\n",
    "\n",
    "# Save\n",
    "csv_path = r\"../output/1000_lakes_temp_pptn_runoff.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "sn.relplot(\n",
    "    data=df,\n",
    "    x=\"year\",\n",
    "    y=\"value\",\n",
    "    row=\"variable\",\n",
    "    height=4,\n",
    "    aspect=3,\n",
    "    kind=\"line\",\n",
    "    legend=False,\n",
    "    alpha=0.5,\n",
    "    facet_kws={\"sharey\": False, \"sharex\": False},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add to database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Add metadata parameters to database\n",
    "\n",
    "I have created new entries in the `station_parameter_definitions` table in RESA, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata IDs\n",
    "sql = (\n",
    "    \"SELECT * FROM resa2.station_parameter_definitions \"\n",
    "    \"WHERE entered_by = 'JES' \"\n",
    "    \"AND entered_date >= DATE '2020-05-26'\"\n",
    ")\n",
    "par_df = pd.read_sql(sql, eng)\n",
    "\n",
    "par_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Extract data of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1995\n",
    "df_1995 = df.query(\"year == 1995\").copy()\n",
    "df_1995[\"var_id\"] = df_1995[\"variable\"].replace({\"t2m\": 341, \"tp\": 342, \"ro\": 343})\n",
    "df_1995 = df_1995[[\"station_id\", \"var_id\", \"value\"]]\n",
    "\n",
    "# 2019\n",
    "df_2019 = df.query(\"year == 2019\").copy()\n",
    "df_2019[\"var_id\"] = df_2019[\"variable\"].replace({\"t2m\": 344, \"tp\": 345, \"ro\": 346})\n",
    "df_2019 = df_2019[[\"station_id\", \"var_id\", \"value\"]]\n",
    "\n",
    "# Average 1981 - 2010\n",
    "df_ltav = df.query(\"1981 <= year <= 2010\").copy()\n",
    "df_ltav = df_ltav.groupby([\"station_id\", \"variable\"]).mean().reset_index()\n",
    "df_ltav[\"var_id\"] = df_ltav[\"variable\"].replace({\"t2m\": 347, \"tp\": 348, \"ro\": 349})\n",
    "df_ltav = df_ltav[[\"station_id\", \"var_id\", \"value\"]]\n",
    "\n",
    "# Average 1990 - 2019\n",
    "df_ltav2 = df.query(\"1990 <= year <= 2019\").copy()\n",
    "df_ltav2 = df_ltav2.groupby([\"station_id\", \"variable\"]).mean().reset_index()\n",
    "df_ltav2[\"var_id\"] = df_ltav2[\"variable\"].replace({\"t2m\": 354, \"tp\": 355, \"ro\": 356})\n",
    "df_ltav2 = df_ltav2[[\"station_id\", \"var_id\", \"value\"]]\n",
    "\n",
    "# Combine\n",
    "stn_par_df = pd.concat([df_1995, df_2019, df_ltav, df_ltav2], axis=0)\n",
    "\n",
    "stn_par_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Add to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stn_par_df.to_sql(\n",
    "#   \"stations_par_values\",\n",
    "#   schema=\"resa2\",\n",
    "#   con=eng,\n",
    "#   if_exists=\"append\",\n",
    "#   index=False,\n",
    "#)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
